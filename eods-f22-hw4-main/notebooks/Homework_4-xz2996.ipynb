{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "### Due: Fri Dec 2nd @ 11:59pm ET\n",
    "\n",
    "# NLP: Recommendations and Sentiment Analysis\n",
    "\n",
    "In this homework we will perform two common NLP tasks: \n",
    " 1. Generate recommendations for products based on product descriptions using an LDA topic model.\n",
    " 2. Perform sentiment analysis based on product reviews using sklearn Pipelines.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Replace Name and UNI in the first cell and filename\n",
    "- Follow the comments below and fill in the blanks (\\_\\_\\_\\_) to complete.\n",
    "- Where not specified, please run functions with default argument settings.\n",
    "- Please **'Restart and Run All'** prior to submission.\n",
    "- **Save pdf in Landscape** and **check that all of your code is shown** in the submission.\n",
    "- When submitting in Gradescope, be sure to **select which page corresponds to which question.**\n",
    "\n",
    "Out of 50 points total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. (2pts total) Homework Submission\n",
    "\n",
    "# (1pt) The homework should be spread over multiple pdf pages, not one single pdf page\n",
    "# (1pt) When submitting, assign each question to the pdf page where the solution is printed.\n",
    "#        If there is no print statement for a question, assign the question to the first pdf \n",
    "#        page where the code for the question is visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Generate Recommendations from LDA Transformation\n",
    "\n",
    "In this part we will transform a set of product descriptions using TfIdf and LDA topic modeling to generate product recommendations based on similarity in LDA space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and transform text using TfIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   name_title   5000 non-null   object\n",
      " 1   description  5000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 78.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# 2. (1pts) Load the Data\n",
    "\n",
    "# The dataset we'll be working with is a set of product descriptions \n",
    "#   from the JCPenney department store.\n",
    "\n",
    "# Load product information from ../data/jcpenney-products_subset.csv.zip\n",
    "# Use pandas read_csv function with the default parameters.\n",
    "# Note that this is a compressed version of a csv file (has a .zip suffix).\n",
    "# .read_csv() has a parameter 'compression' with default \n",
    "#     value 'infer' that will handle unzipping the data for us.\n",
    "# Store the resulting dataframe as df_jcp.\n",
    "df_jcp = pd.read_csv('../data/jcpenney-products_subset.csv.zip',compression='infer' )\n",
    "\n",
    "# print a summary of df_jcp using .info()\n",
    "# there should be 5000 rows with 2 columns with no missing data\n",
    "df_jcp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invicta® Sl Rally Mens Black Leather Strap Chronograph Watch 16012\n",
      "--------------------------------------------------\n",
      "A timepiece you can enjoy every day of the week, this sports car-inspired chronograph watch packs plenty of information into an easy-to-read dial.   Brand: Invicta Dial Color: Black Strap: Black leather Clasp: Buckle Movement: Quartz Water Resistance: 100m Case Width: 48mm Case Thickness: 13.5mm Bracelet Dimensions: 210mm long; 22mm wide Model No.: 16012 Special Features: Stopwatch; 3 multifunction sub dials   Jewelry photos are enlarged to show detail.\n"
     ]
    }
   ],
   "source": [
    "# 3. (2pts) Print an Example\n",
    "\n",
    "# The two columns of the dataframe we're interested in are:\n",
    "#   'name_title' which is the name of the product stored as a string\n",
    "#   'description' which is a description of the product stored as a string\n",
    "#\n",
    "# We'll print out the product in the first row as an example\n",
    "# If we try to print both at the same time, pandas will truncate the strings\n",
    "#   so we'll print them seperately\n",
    "\n",
    "# print the name_title column in row 0 of df_jcp\n",
    "print(df_jcp['name_title'].iloc[0])\n",
    "\n",
    "# printing a line of dashes\n",
    "print('-'*50) \n",
    "\n",
    "# print the desciption column in row 0 of df_jcp\n",
    "print(df_jcp['description'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 5678)\n"
     ]
    }
   ],
   "source": [
    "# 4. (4pts) Transform Descriptions using TfIdf\n",
    "\n",
    "# In order to pass our product descriptions to the LDA model, we first\n",
    "#   need to vectorize from strings to fixed length vectors of floats.\n",
    "# To do this we will transform our documents into a TfIdf representation.\n",
    "\n",
    "# Import TfidfVectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#  Instantiate a TfidfVectorizer that will\n",
    "#    use both unigrams + bigrams\n",
    "#    exclude terms which appear in less than 10 documents\n",
    "#    exclude terms which appear in more than 10% of the documents\n",
    "#    all other parameters leave as default\n",
    "# Store as tfidf\n",
    "tfidf = TfidfVectorizer(ngram_range = (1,2),\n",
    "                       min_df = 10,\n",
    "                       max_df = 0.1)\n",
    "\n",
    "# fit_transform() tfidf on the description column of df_jcp, \n",
    "#    creating the transformed dataset X_tfidf\n",
    "# Store as X_tfidf\n",
    "X_tfidf=tfidf.fit_transform(df_jcp['description'])\n",
    "\n",
    "# Print the shape of X_tfidf (should be 5000 x 5678)\n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['jewelry photos', 'features stopwatch', 'special features',\n",
       "        'model no', 'wide model', '22mm wide', 'long 22mm',\n",
       "        'bracelet dimensions', 'case thickness', 'case width',\n",
       "        'resistance 100m', 'water resistance', 'quartz water',\n",
       "        'movement quartz', 'buckle movement', 'clasp buckle',\n",
       "        'leather clasp', 'black leather', 'strap black', 'black strap',\n",
       "        'color black', 'dial color', 'to read', 'easy to', 'an easy',\n",
       "        'plenty of', 'of the', 'day of', 'every day', 'you can', 'sub',\n",
       "        'stopwatch', 'special', 'no', 'model', 'wide', '22mm',\n",
       "        'dimensions', 'bracelet', '5mm', '13', 'thickness', 'width',\n",
       "        'case', '100m', 'resistance', 'water', 'quartz', 'movement',\n",
       "        'buckle', 'clasp', 'leather', 'strap', 'black', 'color', 'brand',\n",
       "        'dial', 'read', 'into', 'plenty', 'watch', 'chronograph',\n",
       "        'inspired', 'car', 'sports', 'week', 'day', 'every', 'enjoy',\n",
       "        'can'], dtype='<U24')]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5: (1pts) Show The Terms Extracted From Row 0\n",
    "\n",
    "# X_tfidf is a matrix of floats, one row per document, one column per vocab term\n",
    "# We can see what terms were extracted, and kept, for the document at df_jcp row 0\n",
    "#   using the .inverse_transform() function\n",
    "# Print the result of calling:\n",
    "#   the .inverse_transform() function of tfidf on the first row of X_tfidf\n",
    "# You should see an array starting with 'jewelry photos'\n",
    "tfidf.inverse_transform(X_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zipper_pocket', 'zipper_pockets', 'zippered', 'zirconia', 'zone']\n"
     ]
    }
   ],
   "source": [
    "# 6. (3pts) Format Bigrams and Print Sample of Extracted Vocabulary \n",
    "\n",
    "# The learned vocabulary can be retrieved from tfidf as a list using .get_feature_names_out()\n",
    "# Store the extracted vocabulary as vocab\n",
    "vocab = tfidf.get_feature_names_out()\n",
    "\n",
    "# Sklearn joins bigrams with a space character.\n",
    "# To make our output easier to read, replace the spaces in each term in \n",
    "#    vocab (a list of strings) with an underscore.\n",
    "# To do this we can use the string .replace() method.\n",
    "# For example x.replace(' ','_') will replace all ' ' in x with '_'.\n",
    "# Store the result back into vocab\n",
    "\n",
    "vocab = [i.replace(' ','_') for i in vocab]\n",
    "\n",
    "# Print the last 5 terms in the vocab\n",
    "#  The first term printed should be 'zipper_pocket'\n",
    "print(vocab[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform product descriptions into topics and print sample terms from topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 20)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. (3pts) Perform Topic Modeling with LDA\n",
    "\n",
    "# Now that we have our vectorized data, we can use Latent Direchlet Allocation to learn \n",
    "#   per-document topic distributions and per-topic term distributions.\n",
    "# Though the documents are likely composed of more, we'll model our dataset using \n",
    "#     20 topics for ease of printing.\n",
    "\n",
    "# Import LatentDirichletAllocation from sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Instantiate a LatentDirichletAllocation model that will\n",
    "#    produce 20 topics\n",
    "#    use all available cores to train\n",
    "#    random_state=512\n",
    "# Store as lda\n",
    "lda = LatentDirichletAllocation(n_components=20,n_jobs=-1,random_state=512)\n",
    "\n",
    "# Run fit_transform on lda using X_tfidf.\n",
    "# Store the output (the per-document topic distributions) as X_lda\n",
    "X_lda=lda.fit_transform(X_tfidf)\n",
    "\n",
    "# Print the shape of the X_lda (should be 5000 x 20)\n",
    "X_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_0 = [0.01 0.74 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      " 0.16 0.01 0.01 0.01 0.01 0.01]\n",
      "\n",
      "n_topics_assigned_0 = 2\n",
      "\n",
      "assigned_topics_0 = [ 1 14]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. (5pts) Get Assigned Topics for Product at df_jcp row 0\n",
    "\n",
    "# Get the assigned topic proportions for the document at row 0 of X_lda\n",
    "# This will be a list of 20 floats between 0 and 1\n",
    "# Round all values to a precision of 2\n",
    "# Store as theta_0\n",
    "theta_0 = X_lda[0].round(2)\n",
    "print(f'{theta_0 = :}\\n')\n",
    "\n",
    "# LDA will assign a small weight (or proability) to each topic for a document\n",
    "# How many of the topics in theta_0 have a (relatively) large weight (> .01)?\n",
    "# Store in n_topics_assigned_0\n",
    "s = sum(theta_0)\n",
    "n_topics_assigned_0 = sum(i/s > 0.01 for i in theta_0)\n",
    "print(f'{n_topics_assigned_0 = :}\\n')\n",
    "\n",
    "# What are the indices of the assigned topics, sorted descending by the values in theta_0?\n",
    "#  Use np.argsort() to return the indices sorted by value (ascending)\n",
    "#  Use [::-1] to reverse the sorting order (from ascending to descending)\n",
    "#  Return only the first n_assigned_0 indices, those with large probability\n",
    "#  Store as assigned_topics_0\n",
    "#  You should see n_topics_assinged_0 indices\n",
    "assigned_topics_0 = np.argsort(theta_0)[::-1][0:n_topics_assigned_0]\n",
    "print(f'{assigned_topics_0 = :}\\n')\n",
    "\n",
    "# Now that we have the topic indexes, we need to see what each topic looks like\n",
    "#   using the per topic word distrutions stored in lda.components_ (next question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0 : upper heel sole synthetic synthetic_upper\n",
      "Topic  1 : dial case color bracelet strap\n",
      "Topic  2 : rug resistant yes backing pad\n",
      "Topic  3 : upper sole rubber rubber_sole construction\n",
      "Topic  4 : moisture wicking moisture_wicking fabric dri\n",
      "Topic  5 : big through_seat just_below sits_just big_tall\n",
      "Topic  6 : what skin what_it oil it_is\n",
      "Topic  7 : only_imported clean_only only return in_its\n",
      "Topic  8 : measures safe wipe set glass\n",
      "Topic  9 : may sterling diamond jewelry_photos gold\n",
      "Topic 10 : socks nylon support fits fits_shoe\n",
      "Topic 11 : stainless_steel stainless steel cooking oven\n",
      "Topic 12 : short short_sleeves tee crewneck shirt\n",
      "Topic 13 : king comforter set shams pillow\n",
      "Topic 14 : tone gold_tone silver_tone silver tone_metal\n",
      "Topic 15 : garment 25½ fitting garment_is loose_fitting\n",
      "Topic 16 : ci inseam_misses petite misses pop\n",
      "Topic 17 : inseam waist pants zip leg\n",
      "Topic 18 : crochet cute bay graphic_print st\n",
      "Topic 19 : sleeveless line wash_line line_dry dry_imported\n"
     ]
    }
   ],
   "source": [
    "# 9. (5pts) Print Top Topic Terms\n",
    "\n",
    "# To get a sense of what each topic is composed of, we can print the most likely terms for each topic.\n",
    "# We'd like a print statement that looks like this:\n",
    "#     Topic # 0 : socks spandex fits shoe fits_shoe\n",
    "\n",
    "# To make indexing easier, first convert vocab from a list to np.array()\n",
    "# Store back into vocab\n",
    "vocab = np.array(vocab)\n",
    "\n",
    "# assert that vocab is the correct datatype\n",
    "assert type(vocab) is np.ndarray, \"vocab needs to be converted to a numpy array\"\n",
    "\n",
    "# For each topic print f'Topic #{topic_idx:2d} : ' followed by the top 5 most likely terms in that topic.\n",
    "# Hints: \n",
    "#   The per topic term distributions are stored in lda.components_ \n",
    "#      which should be a numpy array with shape (20, 5678)\n",
    "#   Iterate through the rows of lda.components_, one row per topic\n",
    "#   Use np.argsort() to get the sorted indices of the current row of lda.components_\n",
    "#      sorted by the values in that row in ascending order\n",
    "#   Use [::-1] to reverse the order of the sorted indices\n",
    "#   Use numpy array indexing to get the first 5 index values\n",
    "#   Use these indices to get the corresponding terms from vocab\n",
    "#   Join the list of terms with spaces using ' '.join()\n",
    "#   Each print statement should start with f'Topic #{topic_idx:2d} : ' \n",
    "#      where topic_idx is an integer 0 to 19\n",
    "# Each line should look similar to the example shown above\n",
    "\n",
    "# Use as many lines of code as you need\n",
    "topic_term = lda.components_ \n",
    "def top_terms(weights):\n",
    "    return list(vocab[np.argsort(weights)[::-1]][:5])\n",
    "for topic_idx in range(len(topic_term)):\n",
    "    print (f'Topic {topic_idx:2d} :', ' '.join(top_terms(topic_term[topic_idx])) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the description column of row 0, the assigned_topics_0 and \n",
    "# the top terms per topic above, our LDA model seems to have generated\n",
    "# topics that make sense given descriptions of department store goods, \n",
    "# with some a better fit than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate recommendations using topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10. (3pts) Generate Similarity Matrix\n",
    "\n",
    "# We'll use Content-Based Filtering to make recommendations based on a query product.\n",
    "# Each product will be represented by its LDA topic weights learned above (X_lda).\n",
    "# We'd like to recommend similar products in LDA space.\n",
    "# We'll use cosine distance as our measure of similarity, where lower distance means\n",
    "#  more similar.\n",
    "# Note that we're using \"distance\" where lower is better instead of \"similarity\" where higher is better\n",
    "#  as the default sorting is ascending and it makes indexing easier.\n",
    "\n",
    "# Import cosine_distances (not cosine_similarity) from sklearn.metrics.pairwise\n",
    "from sklearn.metrics.pairwise import cosine_distances \n",
    "\n",
    "# Use cosine_distances to generate similarity scores on our X_lda data\n",
    "# Store as distances\n",
    "# NOTE: we only need to pass X_lda in once as an argument,\n",
    "#   the function will calculate pairwise distance between all rows in that matrix\n",
    "distances=cosine_distances(X_lda)\n",
    "\n",
    "# print the shape of the distances matrix (should be 5000 x 5000)\n",
    "distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Invicta® Sl Rally Mens Black Leather Strap Chronograph Watch 16012'\n",
      " 'Seiko® Mens Two-Tone Brown Dial Chronograph Watch SSC142'\n",
      " 'Despicable Me Minions Kids Flashing and Sound Digital Watch'\n",
      " 'Citizen® Eco-Drive® Womens Crystal-Accent Stainless Steel Watch EX1320-54E'\n",
      " 'Womens Crystal-Accent White Lizard Faux Leather Cuff Bangle Watch'\n",
      " 'Star Wars® Stormtrooper Kids Flashing and Sound Digital Watch'\n",
      " 'Casio® Mens Champagne Dial Black Resin Strap Sport Watch MW600F-9AV'\n",
      " 'TKO ORLOGI Womens Crystal-Accent Chain-Link Blue Silicone Strap Stretch Watch'\n",
      " 'Pulsar® Mens Silver-Tone Black Ion Watch PS9273'\n",
      " 'Armitron® ProSport Womens Digital Sport Chronograph Watch 45/7036PNK']\n"
     ]
    }
   ],
   "source": [
    "# 11. (4pts) Find Recommended Products\n",
    "\n",
    "# Let's test our proposed recommendation engine using the product at row 0 in df_jcp.\n",
    "#   The name of this product is \"Invicta® Sl Rally Mens Black Leather Strap Chronograph Watch 16012\"\n",
    "#   Our system will recommend products similiar to this product.\n",
    "\n",
    "# Print the names for the top 10 most similar products to this query.\n",
    "# Suggested way to do this is:\n",
    "#   get the cosine distances from row 0 of the distances matrix\n",
    "#   get the indices of this first row of distances sorted by value ascending using np.argsort()\n",
    "#   get the first 10 indexes from this sorted array of indices\n",
    "#   use those indices to index into df_jcp.name_title \n",
    "#   to get the full string, use .values\n",
    "#   print the resulting array\n",
    "\n",
    "# HINT: The first two products will likely be:\n",
    "#   'Invicta® Sl Rally Mens Black Leather Strap Chronograph Watch 16012',\n",
    "#   'Timex® Easy Reader Womens White Leather Strap Watch T2H3917R',\n",
    "distance_0 = distances[0]\n",
    "indices = np.argsort(distance_0)[:10]\n",
    "print(df_jcp.name_title[indices].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Sentiment Analysis Using Pipelines\n",
    "\n",
    "Here we will train a model to classify positive vs negative sentiment on a set of pet supply product reviews using sklearn Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  10000 non-null  object\n",
      " 1   rating  10000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 156.4+ KB\n",
      "\n",
      "My cats are considerably more happy with this toy...and I don't have to leave the sofa to use it, given the long wand length. yay laziness!!\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# 12. (2pts) Load the Data\n",
    "\n",
    "# The dataset we'll be working with is a set of product reviews\n",
    "#   of pet supply items on Amazon.\n",
    "# This data is taken from https://nijianmo.github.io/amazon/index.html\n",
    "#   \"Justifying recommendations using distantly-labeled reviews and fined-grained aspects\"\n",
    "#   Jianmo Ni, Jiacheng Li, Julian McAuley\n",
    "#   Empirical Methods in Natural Language Processing (EMNLP), 2019\n",
    "\n",
    "# Load product reviews from ../data/amazon-petsupply-reviews_subset.csv.zips\n",
    "# Use pandas read_csv function with the default parameters as in part 1.\n",
    "# Store the resulting dataframe as df_amzn.\n",
    "df_amzn = pd.read_csv('../data/amazon-petsupply-reviews_subset.csv.zip',compression='infer' )\n",
    "\n",
    "# print a summary of df_amzn using .info()\n",
    "# there should be 10000 rows with 2 columns\n",
    "df_amzn.info()\n",
    "\n",
    "# print blank line\n",
    "print() \n",
    "\n",
    "# print the review in the first row of the dataframe as an example\n",
    "print(df_amzn['review'].iloc[0])\n",
    "\n",
    "# print the rating in the first row of the dataframe as an example\n",
    "print(df_amzn['rating'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    0.66\n",
      "4    0.14\n",
      "3    0.09\n",
      "1    0.06\n",
      "2    0.05\n",
      "Name: rating, dtype: float64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True     0.66\n",
       "False    0.34\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 13. (2pts) Transform Target\n",
    "\n",
    "# The ratings are originally in a 5 point scale\n",
    "# We'll turn this into a binary classification task to approximate positive vs negative sentiment\n",
    "\n",
    "# Print the proportions of values seen in the rating column\n",
    "#  using value_counts() with normalize=True\n",
    "#  round to a precision of 2\n",
    "print(df_amzn['rating'].value_counts(normalize=True).round(2))\n",
    "\n",
    "# Create a new binary target by setting\n",
    "#  rows where rating is 5 to True\n",
    "#  rows where rating is not 5 to False\n",
    "# Store in y\n",
    "y = (df_amzn['rating']==5).astype(bool)\n",
    "\n",
    "# print a blank line\n",
    "print()\n",
    "\n",
    "# Print the proportions of values seen in y\n",
    "#  using value_counts() with normalize=True\n",
    "#  round to a precision of 2\n",
    "# True here means a rating of 5 (eg positive)\n",
    "# False means a rating less than 5 (eg negative)\n",
    "y.value_counts(normalize=True).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     0.66\n",
       "False    0.34\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 14. (2pts) Train-test split\n",
    "\n",
    "# Import train_test_split from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split df_amzn.review and y into a train and test set\n",
    "#   using train_test_split\n",
    "#   stratifying by y\n",
    "#   with test_size = .2 \n",
    "#   and random_state = 512\n",
    "# Store as reviews_train,reviews_test,y_train,y_test\n",
    "reviews_train,reviews_test,y_train,y_test = train_test_split(df_amzn.review,\n",
    "                                                             y,\n",
    "                                                             test_size=0.2,\n",
    "                                                             stratify=y,\n",
    "                                                             random_state=512)\n",
    "\n",
    "# print the proportion of values seen in y_train\n",
    "#  round to a precision of 2\n",
    "# visually compare this to the proportion of values seen in y\n",
    "#  to confirm that the class distributions are the same\n",
    "y_train.value_counts(normalize=True).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tfidf', TfidfVectorizer(max_df=0.5, min_df=5)),\n",
      "                ('gbc', GradientBoostingClassifier(n_estimators=20))])\n"
     ]
    }
   ],
   "source": [
    "# 15. (4pts) Create a Pipeline of TfIdf transformation and Classification\n",
    "\n",
    "# import Pipeline and GradientBoostingClassifier from sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create a pipeline with two steps: \n",
    "#  TfIdfVectorizer with min_df=5 and max_df=.5 named 'tfidf'\n",
    "#  GradientBoostingClassifier with 20 trees named 'gbc'\n",
    "# Store as pipe_gbc\n",
    "pipe_gbc = Pipeline([('tfidf',TfidfVectorizer(min_df=5, max_df=.5)),\n",
    "                     ('gbc',GradientBoostingClassifier(n_estimators=20))\n",
    "\n",
    "])\n",
    "# Print out the pipeline\n",
    "# You should see both steps: tfidf and gbc\n",
    "print(pipe_gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gbc__max_depth': 10, 'tfidf__ngram_range': [1, 2]}\n",
      "0.74\n"
     ]
    }
   ],
   "source": [
    "# 16. (5pts) Perform Grid Search on pipe_gbc\n",
    "\n",
    "# import GridSearchCV from sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a parameter grid to test using:\n",
    "#   unigrams or unigrams + bigrams in the tfidf step\n",
    "#   max_depth of 2 or 10 in the gbc step\n",
    "# Store as param_grid\n",
    "param_grid = {'tfidf__ngram_range':[[1,1],[1,2]],\n",
    "               'gbc__max_depth':[2,10]}\n",
    "\n",
    "# Instantiate GridSearchCV to evaluate pipe_gbc on the values in param_grid\n",
    "#   use cv=2 and n_jobs=-1 to reduce run time\n",
    "# Fit on the training set of reviews_train,y_train\n",
    "# Store as gs_pipe_gbc\n",
    "gs_pipe_gbc = GridSearchCV(pipe_gbc, param_grid, cv=2, n_jobs=-1).fit(reviews_train,y_train)\n",
    "\n",
    "# Print the best parameter settings in gs_pipe_gbc found by grid search\n",
    "print(gs_pipe_gbc.best_params_)\n",
    "# Print the best cv score found by grid search, with a precision of 2\n",
    "print(gs_pipe_gbc.best_score_.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "# 17. (1 pts) Evaluate on the test set\n",
    "\n",
    "# Calculate the test set (reviews_test,y_test) score using the trained gs_pipe_gbc \n",
    "#   to give confidence that we have not overfit\n",
    "#   while still improving over a random baseline classifier\n",
    "# Print the accuracy score on the test set with a precision of 2\n",
    "print(gs_pipe_gbc.score(reviews_test,y_test).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True]\n",
      "[False]\n"
     ]
    }
   ],
   "source": [
    "# 18. (1 pts) Evaluate on example reviews\n",
    "\n",
    "# Generate predictions for these two sentences using the fit gs_pipe_gbc:\n",
    "#   'This is a great product.'\n",
    "#   'This product is not great.'\n",
    "# You should see True for the first (rating of 5) \n",
    "#   and False for the second (rating of less than 5)\n",
    "print(gs_pipe_gbc.predict({'This is a great product.'}))\n",
    "print(gs_pipe_gbc.predict({'This product is not great.'}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eods-f22",
   "language": "python",
   "name": "eods-f22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
